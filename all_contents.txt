
--- File: C:\Shashank\SaaS\code-rag-debugger\backend\.env ---

DATABASE_URL=postgresql://postgres:Bunny%401018@db.gmesbgksnfvlepjksg
SUPABASE_URL=https://
SUPABASE_KEY=sb_publishable
SECURITY_KEY=sb_secret
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=10080
GEMINI_API_KEY=AIzaSyB

# DeepInfra Configuration
DEEPINFRA_API_KEY=
DEEPINFRA_MODEL=google/gemma-3-27b-it

# Pinecone Configuration
PINECONE_API_KEY=pcsk_6fGXP7_TUPAo5HTxovwnnHUeDR3w3LAscD8jFX
PINECONE_ENVIRONMENT=us-east-1  # e.g., "us-west1-gcp"
PINECONE_INDEX_NAME=rodeceview


OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma:2b
REPOSITORY_BASE_PATH=./repositories
CHROMA_DB_PATH=./chroma_db
EMBEDDING_MODEL=all-MiniLM-L6-v2
BACKEND_CORS_ORIGINS=["http://localhost:3000", "http://localhost:8000", "https://rodeceview.vercel.app"]

# Git Configuration
MAX_COMMIT_HISTORY=1000


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\.gitignore ---

# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.js

# testing
/coverage

# production
/build

# misc
.DS_Store
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

npm-debug.log*
yarn-debug.log*
yarn-error.log*



--- File: C:\Shashank\SaaS\code-rag-debugger\backend\config.py ---

import os
from pydantic_settings import BaseSettings
from typing import List, Optional, Union
from dotenv import load_dotenv
from pydantic import field_validator

load_dotenv()

class Settings(BaseSettings):
    PROJECT_NAME: str = "RodeCeview"

    # Database Configuration
    DATABASE_URL: str = os.getenv("DATABASE_URL", "")
    
    # Supabase Configuration
    SUPABASE_URL: str = os.getenv("SUPABASE_URL", "")
    SUPABASE_KEY: str = os.getenv("SUPABASE_KEY", "")

    # Vector Database - Pinecone
    PINECONE_API_KEY: str = os.getenv("PINECONE_API_KEY", "")
    PINECONE_ENVIRONMENT: str = os.getenv("PINECONE_ENVIRONMENT", "")
    PINECONE_INDEX_NAME: str = os.getenv("PINECONE_INDEX_NAME", "code-review-index")

    # Gemini API Key (Replaces DeepInfra)
    GEMINI_API_KEY: str = os.getenv("GEMINI_API_KEY", "")
    
    # Embedding Model
    EMBEDDING_MODEL: str = os.getenv("EMBEDDING_MODEL", "all-MiniLM-L6-v2")

    # Git Configuration
    MAX_COMMIT_HISTORY: int = int(os.getenv("MAX_COMMIT_HISTORY", 1000))
    REPOSITORY_BASE_PATH: str = os.getenv("REPOSITORY_BASE_PATH", "./repositories")

    # Security
    SECRET_KEY: str = os.getenv("SECRET_KEY", "your-secret-key-here")
    ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 7  # 7 days

    # CORS
    BACKEND_CORS_ORIGINS: List[str] = ["http://localhost:3000", "http://localhost:8000", "https://rodeceview.vercel.app"]

    @field_validator("BACKEND_CORS_ORIGINS", mode="before")
    def parse_cors_origins(cls, v: Union[str, List[str]]) -> List[str]:
        if isinstance(v, str):
            # allow comma-separated list in env
            return [i.strip() for i in v.split(",") if i.strip()]
        return v

    class Config:
        case_sensitive = True

settings = Settings()



--- File: C:\Shashank\SaaS\code-rag-debugger\backend\docker-compose.yml ---

version: '3.8'
services:
  backend:
    build: ./
    restart: always
    env_file:
      - .env
    ports:
      - "8000:8000"
    volumes:
      - ./repositories:/repositories
      - ./logs:/app/logs  # Optional: for persistent logs
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
    # Health check (optional)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

# Note: Removed ollama and chroma volumes since we're using external services
# (DeepInfra API and Pinecone) that don't require local containers


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\Dockerfile ---

FROM python:3.11-slim

# Install system dependencies (git + build essentials)
RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy app source code
COPY . .

# Expose port
EXPOSE 8000

# Start the app with uvicorn
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\main.py ---

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging

from config import settings
from routers import projects, debug, history, references

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("Starting up application...")
    yield
    logger.info("Shutting down...")

app = FastAPI(
    title=settings.PROJECT_NAME,
    description="Code Evolution Tracker & Debugger with RAG",
    version="1.0.0",
    lifespan=lifespan
)

# FIXED CORS CONFIGURATION
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://rodeceview.vercel.app", "http://localhost:3000"],  # Your React frontend URL
    allow_credentials=True,
    allow_methods=["*"],  # Allow all methods including OPTIONS
    allow_headers=["*"],  # Allow all headers
)

# Include routers
app.include_router(projects.router)
app.include_router(debug.router)
app.include_router(history.router)
app.include_router(references.router)

@app.get("/")
async def root():
    return {"message": "Code Evolution Tracker & Debugger API"}

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\output.py ---

import os

ALLOWED_FOLDERS = {"migrations", "models", "routers", "services", "utils"}
EXCLUDED_FOLDERS = {"__pycache__", "venv", "repositories"}

def collect_contents(root_folder, output_file):
    with open(output_file, "w", encoding="utf-8") as outfile:
        for foldername, subfolders, filenames in os.walk(root_folder):
            # Remove excluded folders so os.walk won't enter them
            subfolders[:] = [sf for sf in subfolders if sf not in EXCLUDED_FOLDERS]

            # relative path from root
            rel_path = os.path.relpath(foldername, root_folder)

            for filename in filenames:
                # Skip the output file itself if inside root folder
                if filename == output_file:
                    continue

                file_path = os.path.join(foldername, filename)

                # Check if file is in root or in allowed folders
                if (
                    rel_path == "."  # file is in root folder
                    or rel_path.split(os.sep)[0] in ALLOWED_FOLDERS
                ):
                    try:
                        with open(file_path, "r", encoding="utf-8") as infile:
                            outfile.write(f"\n--- File: {file_path} ---\n\n")
                            outfile.write(infile.read())
                            outfile.write("\n\n")
                    except Exception as e:
                        outfile.write(f"\n--- Could not read {file_path}: {e} ---\n\n")

if __name__ == "__main__":
    root_folder = os.getcwd()  # current directory
    output_file = "all_contents.txt"
    collect_contents(root_folder, output_file)
    print(f"âœ… All contents written to {output_file}")



--- File: C:\Shashank\SaaS\code-rag-debugger\backend\requirements.txt ---

fastapi==0.104.1
uvicorn==0.24.0
python-multipart==0.0.6
sqlalchemy==2.0.23
alembic==1.12.1
psycopg2-binary==2.9.9
chromadb==0.4.15
sentence-transformers==2.2.2
requests==2.31.0
gitpython==3.1.37
python-jose==3.3.0
passlib==1.7.4
python-dotenv==1.0.0
pydantic==2.5.0
pydantic-settings==2.1.0
aiofiles==23.2.1
watchdog==3.0.0
huggingface-hub==0.16.4
supabase
pinecone  # Changed from pinecone-client
tree_sitter


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\__init__.py ---




--- File: C:\Shashank\SaaS\code-rag-debugger\backend\migrations\001_add_symbol_tables.sql ---

-- Migration to add symbol and reference tables
CREATE TABLE symbols (
    id SERIAL PRIMARY KEY,
    project_id INTEGER NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    symbol_name VARCHAR(512) NOT NULL,
    symbol_type VARCHAR(50) NOT NULL CHECK (symbol_type IN ('function', 'class', 'method', 'constant', 'variable', 'module', 'import')),
    language VARCHAR(50) NOT NULL,
    file_path VARCHAR(1024) NOT NULL,
    start_line INTEGER NOT NULL,
    end_line INTEGER NOT NULL,
    signature TEXT,
    docstring TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    commit_hash VARCHAR(64),
    token_count_estimate INTEGER DEFAULT 0,
    usage_count INTEGER DEFAULT 0,
    centrality_score FLOAT DEFAULT 0.0,
    metadata JSONB DEFAULT '{}'::jsonb,
    UNIQUE(project_id, file_path, symbol_name, start_line)
);

CREATE INDEX idx_symbols_project_id ON symbols(project_id);
CREATE INDEX idx_symbols_file_path ON symbols(file_path);
CREATE INDEX idx_symbols_symbol_name ON symbols(symbol_name);
CREATE INDEX idx_symbols_symbol_type ON symbols(symbol_type);
CREATE INDEX idx_symbols_language ON symbols(language);

CREATE TABLE symbol_chunks (
    id SERIAL PRIMARY KEY,
    symbol_id INTEGER NOT NULL REFERENCES symbols(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    start_line INTEGER NOT NULL,
    end_line INTEGER NOT NULL,
    embedding_vector_id VARCHAR(256),
    token_count INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(symbol_id, chunk_index)
);

CREATE INDEX idx_symbol_chunks_symbol_id ON symbol_chunks(symbol_id);

CREATE TABLE references (
    id SERIAL PRIMARY KEY,
    from_symbol_id INTEGER NOT NULL REFERENCES symbols(id) ON DELETE CASCADE,
    to_symbol_id INTEGER NOT NULL REFERENCES symbols(id) ON DELETE CASCADE,
    reference_type VARCHAR(50) NOT NULL CHECK (reference_type IN ('call', 'import', 'inheritance', 'usage', 'implementation')),
    file_path VARCHAR(1024) NOT NULL,
    line INTEGER NOT NULL,
    context_snippet TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(from_symbol_id, to_symbol_id, reference_type, file_path, line)
);

CREATE INDEX idx_references_from_symbol ON references(from_symbol_id);
CREATE INDEX idx_references_to_symbol ON references(to_symbol_id);
CREATE INDEX idx_references_reference_type ON references(reference_type);

CREATE TABLE symbol_embeddings_metadata (
    id SERIAL PRIMARY KEY,
    symbol_id INTEGER NOT NULL REFERENCES symbols(id) ON DELETE CASCADE,
    pinecone_id VARCHAR(256) NOT NULL,
    namespace VARCHAR(256) NOT NULL,
    embedding_dim INTEGER NOT NULL,
    last_upserted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(symbol_id, namespace)
);

CREATE INDEX idx_symbol_embeddings_symbol ON symbol_embeddings_metadata(symbol_id);
CREATE INDEX idx_symbol_embeddings_pinecone ON symbol_embeddings_metadata(pinecone_id);

CREATE TABLE indexing_jobs (
    id SERIAL PRIMARY KEY,
    project_id INTEGER NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    status VARCHAR(50) NOT NULL CHECK (status IN ('pending', 'processing', 'completed', 'failed')),
    started_at TIMESTAMP WITH TIME ZONE,
    finished_at TIMESTAMP WITH TIME ZONE,
    stats JSONB DEFAULT '{}'::jsonb,
    last_error TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_indexing_jobs_project ON indexing_jobs(project_id);
CREATE INDEX idx_indexing_jobs_status ON indexing_jobs(status);


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\migrations\fix_null_dates.py ---

# migrations/fix_null_dates.py
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from config import settings

def fix_null_dates():
    engine = create_engine(settings.DATABASE_URL)
    Session = sessionmaker(bind=engine)
    session = Session()
    
    try:
        # Fix projects with NULL created_at or updated_at
        session.execute(text("""
            UPDATE projects 
            SET created_at = CURRENT_TIMESTAMP 
            WHERE created_at IS NULL
        """))
        
        session.execute(text("""
            UPDATE projects 
            SET updated_at = CURRENT_TIMESTAMP 
            WHERE updated_at IS NULL
        """))
        
        # Fix commits with NULL timestamp
        session.execute(text("""
            UPDATE commits 
            SET timestamp = CURRENT_TIMESTAMP 
            WHERE timestamp IS NULL
        """))
        
        # Fix feedback with NULL created_at
        session.execute(text("""
            UPDATE feedback 
            SET created_at = CURRENT_TIMESTAMP 
            WHERE created_at IS NULL
        """))
        
        session.commit()
        print("Successfully fixed NULL date values")
        
    except Exception as e:
        session.rollback()
        print(f"Error fixing NULL dates: {e}")
    finally:
        session.close()

if __name__ == "__main__":
    fix_null_dates()


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\migrations\__init__.py ---




--- File: C:\Shashank\SaaS\code-rag-debugger\backend\models\commit.py ---

from datetime import datetime
from pydantic import BaseModel
from typing import List, Optional

class Commit(BaseModel):
    id: Optional[int] = None
    hash: str
    author: str
    message: str
    timestamp: datetime
    files_changed: List[str]
    project_id: int

    class Config:
        from_attributes = True


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\models\crud.py ---

from typing import Type, TypeVar, List, Optional, Dict, Any
from models.database import get_db
from supabase import Client
import logging

logger = logging.getLogger(__name__)

ModelType = TypeVar("ModelType")

class CRUDBase:
    def __init__(self, table_name: str):
        self.table_name = table_name

    def get(self, id: int) -> Optional[Dict[str, Any]]:
        try:
            supabase = get_db()
            result = supabase.table(self.table_name).select("*").eq("id", id).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f"Error getting record from {self.table_name}: {e}")
            return None

    def get_multi(self, skip: int = 0, limit: int = 100) -> List[Dict[str, Any]]:
        try:
            supabase = get_db()
            result = supabase.table(self.table_name).select("*").range(skip, skip + limit - 1).execute()
            return result.data
        except Exception as e:
            logger.error(f"Error getting multiple records from {self.table_name}: {e}")
            return []

    def create(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        try:
            supabase = get_db()
            result = supabase.table(self.table_name).insert(data).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f"Error creating record in {self.table_name}: {e}")
            return None

    def update(self, id: int, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        try:
            supabase = get_db()
            result = supabase.table(self.table_name).update(data).eq("id", id).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f"Error updating record in {self.table_name}: {e}")
            return None

    def remove(self, id: int) -> Optional[Dict[str, Any]]:
        try:
            supabase = get_db()
            result = supabase.table(self.table_name).delete().eq("id", id).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f"Error deleting record from {self.table_name}: {e}")
            return None

class CRUDProject(CRUDBase):
    def __init__(self):
        super().__init__("projects")

    def get_by_name(self, name: str) -> Optional[Dict[str, Any]]:
        try:
            supabase = get_db()
            result = supabase.table("projects").select("*").eq("name", name).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f"Error getting project by name: {e}")
            return None

class CRUDCommit(CRUDBase):
    def __init__(self):
        super().__init__("commits")

    def get_by_hash(self, hash: str, project_id: int) -> Optional[Dict[str, Any]]:
        try:
            supabase = get_db()
            result = supabase.table("commits").select("*").eq("hash", hash).eq("project_id", project_id).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f"Error getting commit by hash: {e}")
            return None

    def get_by_project(self, project_id: int, skip: int = 0, limit: int = 100) -> List[Dict[str, Any]]:
        try:
            supabase = get_db()
            result = supabase.table("commits").select("*").eq("project_id", project_id).range(skip, skip + limit - 1).execute()
            return result.data
        except Exception as e:
            logger.error(f"Error getting commits by project: {e}")
            return []

    def create_commit(self, commit_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        try:
            supabase = get_db()
            result = supabase.table("commits").insert(commit_data).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f"Error creating commit: {e}")
            return None

class CRUDFeedback(CRUDBase):
    def __init__(self):
        super().__init__("feedback")

    def get_by_debug_query(self, debug_query_id: int) -> List[Dict[str, Any]]:
        try:
            supabase = get_db()
            result = supabase.table("feedback").select("*").eq("debug_query_id", debug_query_id).execute()
            return result.data
        except Exception as e:
            logger.error(f"Error getting feedback by debug query: {e}")
            return []

# Add these methods to the existing CRUDBase class

class CRUDSymbol(CRUDBase):
    def __init__(self):
        super().__init__("symbols")

    def get_by_file_and_line(self, project_id: int, file_path: str, start_line: int, end_line: int = None) -> List[Dict[str, Any]]:
        """Get symbols by file and line range"""
        try:
            supabase = get_db()
            query = supabase.table("symbols").select("*").eq("project_id", project_id).eq("file_path", file_path)
            
            if end_line:
                query = query.gte("start_line", start_line).lte("end_line", end_line)
            else:
                query = query.eq("start_line", start_line)
                
            result = query.execute()
            return result.data
        except Exception as e:
            logger.error(f"Error getting symbols by file and line: {e}")
            return []

    def get_by_unique(self, project_id: int, file_path: str, symbol_name: str, start_line: int) -> Optional[Dict[str, Any]]:
        """Get symbol by unique combination"""
        try:
            supabase = get_db()
            result = supabase.table("symbols").select("*").eq("project_id", project_id).eq("file_path", file_path).eq("symbol_name", symbol_name).eq("start_line", start_line).execute()
            return result.data[0] if result.data else None
        except Exception as e:
            logger.error(f"Error getting symbol by unique: {e}")
            return None

    def get_by_project(self, project_id: int, skip: int = 0, limit: int = 100) -> List[Dict[str, Any]]:
        """Get symbols by project"""
        try:
            supabase = get_db()
            result = supabase.table("symbols").select("*").eq("project_id", project_id).range(skip, skip + limit - 1).execute()
            return result.data
        except Exception as e:
            logger.error(f"Error getting symbols by project: {e}")
            return []

class CRUDSymbolChunk(CRUDBase):
    def __init__(self):
        super().__init__("symbol_chunks")

    def get_by_symbol(self, symbol_id: int) -> List[Dict[str, Any]]:
        """Get chunks by symbol ID"""
        try:
            supabase = get_db()
            result = supabase.table("symbol_chunks").select("*").eq("symbol_id", symbol_id).execute()
            return result.data
        except Exception as e:
            logger.error(f"Error getting symbol chunks: {e}")
            return []

class CRUDReference(CRUDBase):
    def __init__(self):
        super().__init__("references")

    def get_by_symbol(self, symbol_id: int) -> List[Dict[str, Any]]:
        """Get references by symbol ID"""
        try:
            supabase = get_db()
            result = supabase.table("references").select("*").eq("from_symbol_id", symbol_id).execute()
            return result.data
        except Exception as e:
            logger.error(f"Error getting references by symbol: {e}")
            return []

    def get_to_symbol(self, symbol_id: int) -> List[Dict[str, Any]]:
        """Get references pointing to symbol ID"""
        try:
            supabase = get_db()
            result = supabase.table("references").select("*").eq("to_symbol_id", symbol_id).execute()
            return result.data
        except Exception as e:
            logger.error(f"Error getting references to symbol: {e}")
            return []

class CRUDSymbolEmbedding(CRUDBase):
    def __init__(self):
        super().__init__("symbol_embeddings_metadata")

    def get_by_symbol(self, symbol_id: int) -> List[Dict[str, Any]]:
        """Get embeddings by symbol ID"""
        try:
            supabase = get_db()
            result = supabase.table("symbol_embeddings_metadata").select("*").eq("symbol_id", symbol_id).execute()
            return result.data
        except Exception as e:
            logger.error(f"Error getting symbol embeddings: {e}")
            return []

class CRUDIndexingJob(CRUDBase):
    def __init__(self):
        super().__init__("indexing_jobs")

    def get_by_project(self, project_id: int) -> List[Dict[str, Any]]:
        """Get indexing jobs by project"""
        try:
            supabase = get_db()
            result = supabase.table("indexing_jobs").select("*").eq("project_id", project_id).order("created_at", desc=True).execute()
            return result.data
        except Exception as e:
            logger.error(f"Error getting indexing jobs: {e}")
            return []

# Add these to the existing CRUD instances
symbol = CRUDSymbol()
symbol_chunk = CRUDSymbolChunk()
reference = CRUDReference()
symbol_embedding = CRUDSymbolEmbedding()
indexing_job = CRUDIndexingJob()
project = CRUDProject()
commit = CRUDCommit()
feedback = CRUDFeedback()


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\models\database.py ---

from supabase import create_client, Client
from config import settings
import logging

logger = logging.getLogger(__name__)

# Initialize Supabase client only if URL and KEY are provided
supabase: Client = None

def initialize_supabase():
    global supabase
    if settings.SUPABASE_URL and settings.SUPABASE_KEY:
        try:
            # Validate URL format
            if not settings.SUPABASE_URL.startswith(('http://', 'https://')):
                logger.error(f"Invalid Supabase URL format: {settings.SUPABASE_URL}")
                return False
            
            supabase = create_client(settings.SUPABASE_URL, settings.SUPABASE_KEY)
            logger.info("Supabase client initialized successfully")
            return True
        except Exception as e:
            logger.error(f"Failed to initialize Supabase client: {e}")
            supabase = None
            return False
    else:
        logger.warning("Supabase URL or KEY not configured. Supabase functionality will be disabled.")
        return False

# Initialize on import
supabase_initialized = initialize_supabase()

def get_db():
    """
    Returns Supabase client for queries.
    """
    if supabase is None:
        raise Exception("Supabase client not initialized. Check your SUPABASE_URL and SUPABASE_KEY configuration.")
    return supabase


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\models\feedback.py ---

from datetime import datetime
from pydantic import BaseModel
from typing import Optional

class Feedback(BaseModel):
    id: Optional[int] = None
    debug_query_id: int
    helpful: bool = False
    comments: Optional[str] = None
    created_at: Optional[datetime] = None

    class Config:
        from_attributes = True


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\models\models.py ---

from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime

class Project(BaseModel):
    id: Optional[int] = None
    name: str
    git_url: str
    description: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    class Config:
        from_attributes = True

class Commit(BaseModel):
    id: Optional[int] = None
    hash: str
    author: str
    message: str
    timestamp: datetime
    files_changed: List[str]
    project_id: int

    class Config:
        from_attributes = True

class Feedback(BaseModel):
    id: Optional[int] = None
    debug_query_id: int
    helpful: bool
    comments: Optional[str] = None
    created_at: Optional[datetime] = None

    class Config:
        from_attributes = True


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\models\project.py ---

from datetime import datetime
from pydantic import BaseModel
from typing import Optional, List
from .commit import Commit

class Project(BaseModel):
    id: Optional[int] = None
    name: str
    git_url: str
    description: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    commits: List[Commit] = []

    class Config:
        from_attributes = True


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\models\schemas.py ---

from pydantic import BaseModel, HttpUrl
from typing import List, Optional, Dict, Any
from datetime import datetime
from .symbol import Symbol, Reference, SymbolChunk, SymbolEmbeddingMetadata, IndexingJob, ReferencePack

# Project schemas
class ProjectBase(BaseModel):
    name: str
    git_url: str
    description: Optional[str] = None

class ProjectCreate(ProjectBase):
    pass

class Project(ProjectBase):
    id: int
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    
    class Config:
        from_attributes = True

# Commit schemas
class CommitBase(BaseModel):
    hash: str
    author: str
    message: str
    timestamp: Optional[datetime] = None
    files_changed: List[str]

class CommitCreate(CommitBase):
    project_id: int

class Commit(CommitBase):
    id: int
    project_id: int
    
    class Config:
        from_attributes = True

# Debug query schemas
class DebugQuery(BaseModel):
    error_message: str
    code_snippet: Optional[str] = None
    file_path: Optional[str] = None
    additional_context: Optional[str] = None
    project_id: int
    use_reference_pack: bool = True  # New parameter

class DebugContext(BaseModel):
    commit: Optional[Commit] = None
    similar_errors: List[Any] = []
    documentation: List[str] = []
    code_suggestions: List[str] = []

class DebugResponse(BaseModel):
    solution: str
    context: DebugContext
    confidence: float

# RAG schemas
class RAGQuery(BaseModel):
    query: str
    project_id: int
    max_results: int = 5

class RAGResult(BaseModel):
    content: str
    source: str
    similarity: float

class RAGResponse(BaseModel):
    results: List[RAGResult]
    answer: str

# User feedback schemas
class FeedbackBase(BaseModel):
    debug_query_id: int
    helpful: bool
    comments: Optional[str] = None

class FeedbackCreate(FeedbackBase):
    pass

class Feedback(FeedbackBase):
    id: int
    created_at: Optional[datetime] = None
    
    class Config:
        from_attributes = True

# Reference schemas - moved to avoid circular imports
class DebugReferenceRequest(BaseModel):
    project_id: int
    error_snippet: str
    file_path: str
    start_line: Optional[int] = None
    end_line: Optional[int] = None
    token_budget: int = 4000
    ranking_params: Optional[Dict[str, float]] = None

# Forward references for symbol-related schemas
class SymbolSearchResult(BaseModel):
    symbol: 'Symbol'  # Forward reference
    similarity: float
    metadata: Dict[str, Any]

class ReferenceWithSymbol(BaseModel):
    reference: 'Reference'  # Forward reference
    depth: int
    symbol: 'Symbol'  # Forward reference
    
class ReferencePack(BaseModel):
    symbol: Symbol
    references: List[Reference]
    ranking_params: Optional[Dict[str, float]] = None
    token_budget: Optional[int] = None


# Import symbol models here to avoid circular imports
from .symbol import Symbol, Reference

# Update the forward references with actual classes
SymbolSearchResult.update_forward_refs()
ReferenceWithSymbol.update_forward_refs()


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\models\symbol.py ---

from datetime import datetime
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from enum import Enum

class SymbolType(str, Enum):
    FUNCTION = "function"
    CLASS = "class"
    METHOD = "method"
    CONSTANT = "constant"
    VARIABLE = "variable"
    MODULE = "module"
    IMPORT = "import"

class ReferenceType(str, Enum):
    CALL = "call"
    IMPORT = "import"
    INHERITANCE = "inheritance"
    USAGE = "usage"
    IMPLEMENTATION = "implementation"

class SymbolBase(BaseModel):
    project_id: int
    symbol_name: str
    symbol_type: SymbolType
    language: str
    file_path: str
    start_line: int
    end_line: int
    signature: Optional[str] = None
    docstring: Optional[str] = None
    commit_hash: Optional[str] = None
    token_count_estimate: int = 0
    usage_count: int = 0
    centrality_score: float = 0.0
    metadata: Dict[str, Any] = Field(default_factory=dict)

class SymbolCreate(SymbolBase):
    pass

class Symbol(SymbolBase):
    id: int
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    class Config:
        from_attributes = True

class SymbolChunk(BaseModel):
    id: Optional[int] = None
    symbol_id: int
    chunk_index: int
    content: str
    start_line: int
    end_line: int
    embedding_vector_id: Optional[str] = None
    token_count: int = 0
    created_at: Optional[datetime] = None

    class Config:
        from_attributes = True

class Reference(BaseModel):
    id: Optional[int] = None
    from_symbol_id: int
    to_symbol_id: int
    reference_type: ReferenceType
    file_path: str
    line: int
    context_snippet: Optional[str] = None
    created_at: Optional[datetime] = None

    class Config:
        from_attributes = True

class SymbolEmbeddingMetadata(BaseModel):
    id: Optional[int] = None
    symbol_id: int
    pinecone_id: str
    namespace: str
    embedding_dim: int
    last_upserted_at: Optional[datetime] = None

    class Config:
        from_attributes = True

class IndexingJob(BaseModel):
    id: Optional[int] = None
    project_id: int
    status: str
    started_at: Optional[datetime] = None
    finished_at: Optional[datetime] = None
    stats: Dict[str, Any] = Field(default_factory=dict)
    last_error: Optional[str] = None
    created_at: Optional[datetime] = None

    class Config:
        from_attributes = True

class ReferencePack(BaseModel):
    symbol: Symbol
    definition: str
    references: List[Dict[str, Any]]
    callers: List[Dict[str, Any]]
    callees: List[Dict[str, Any]]
    imports: List[Dict[str, Any]]
    tests: List[Dict[str, Any]]
    historical_fixes: List[Dict[str, Any]]
    token_count: int
    reasoning: str

    class Config:
        from_attributes = True


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\models\__init__.py ---

from .database import get_db
from .models import Project, Commit, Feedback
from . import schemas
from . import crud

# Import symbol models after other models to avoid circular imports
from .symbol import Symbol, Reference, SymbolChunk, SymbolEmbeddingMetadata, IndexingJob, ReferencePack

__all__ = [
    "get_db", 
    "Project", "Commit", "Feedback",
    "Symbol", "Reference", "SymbolChunk", "SymbolEmbeddingMetadata", "IndexingJob", "ReferencePack",
    "schemas", "crud"
]


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\routers\debug.py ---

from fastapi import APIRouter, Depends, HTTPException, status
from models.database import get_db
from models import schemas
from models import crud
from services.rag_service import rag_service

router = APIRouter(prefix="/debug", tags=["debug"])

@router.post("/", response_model=schemas.DebugResponse)
def debug_code(query: schemas.DebugQuery):
    """Debug a code error using RAG"""
    # Verify project exists
    db_project = crud.project.get(query.project_id)
    if db_project is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )
    
    # Generate debug response using RAG
    result = rag_service.generate_debug_response(
        query.project_id,
        query.error_message,
        query.code_snippet,
        query.file_path,
        query.additional_context,
        query.use_reference_pack
    )
    
    return result

@router.post("/rag", response_model=schemas.RAGResponse)
def rag_query(query: schemas.RAGQuery):
    """Perform a general RAG query on project data"""
    # Verify project exists
    db_project = crud.project.get(query.project_id)
    if db_project is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )
    
    # This would use the retrieval service to get results
    # For now, return a placeholder response
    return {
        "results": [],
        "answer": "This feature is not fully implemented yet."
    }


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\routers\history.py ---

from fastapi import APIRouter, Depends, HTTPException, status
from typing import List

from models import schemas
from models import crud

router = APIRouter(prefix="/history", tags=["history"])

@router.get("/commits/{commit_hash}", response_model=schemas.Commit)
def read_commit(commit_hash: str, project_id: int):
    """Get a specific commit"""
    commit = crud.commit.get_by_hash(hash=commit_hash, project_id=project_id)
    if commit is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Commit not found"
        )
    return commit

@router.post("/feedback", response_model=schemas.Feedback)
def create_feedback(feedback: schemas.FeedbackCreate):
    """Submit feedback on a debug response"""
    db_feedback = crud.feedback.create(feedback.model_dump())
    return db_feedback

@router.get("/feedback/{debug_query_id}", response_model=List[schemas.Feedback])
def read_feedback(debug_query_id: int):
    """Get feedback for a specific debug query"""
    feedback = crud.feedback.get_by_debug_query(debug_query_id=debug_query_id)
    return feedback


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\routers\projects.py ---

from fastapi import APIRouter, Depends, HTTPException, status, Response
from typing import List
import logging

from models import schemas
from models import crud
from services.git_service import git_service
from services.embedding_service import embedding_service
from models.database import get_db

router = APIRouter(prefix="/projects", tags=["projects"])
logger = logging.getLogger(__name__)

# --- CORS Preflight Handlers ---
@router.options("/")
async def options_projects():
    return Response(headers={
        "Access-Control-Allow-Origin": "https://rodeceview.vercel.app",
        "Access-Control-Allow-Methods": "POST, GET, OPTIONS",
        "Access-Control-Allow-Headers": "Content-Type",
    })

@router.options("/{project_id}")
async def options_project(project_id: int):
    return Response(headers={
        "Access-Control-Allow-Origin": "https://rodeceview.vercel.app",
        "Access-Control-Allow-Methods": "GET, OPTIONS",
        "Access-Control-Allow-Headers": "Content-Type",
    })

@router.options("/{project_id}/refresh")
async def options_refresh(project_id: int):
    return Response(headers={
        "Access-Control-Allow-Origin": "https://rodeceview.vercel.app",
        "Access-Control-Allow-Methods": "POST, OPTIONS",
        "Access-Control-Allow-Headers": "Content-Type",
    })

@router.options("/{project_id}/commits")
async def options_commits(project_id: int):
    return Response(headers={
        "Access-Control-Allow-Origin": "https://rodeceview.vercel.app",
        "Access-Control-Allow-Methods": "GET, OPTIONS",
        "Access-Control-Allow-Headers": "Content-Type",
    })

# --- Existing Endpoints ---
@router.post("/", response_model=schemas.Project)
def create_project(project: schemas.ProjectCreate):
    """Create a new project and clone its repository"""
    try:
        # Test database connection first
        get_db()
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Database connection failed: {str(e)}"
        )
    
    # Check if project already exists
    db_project = crud.project.get_by_name(name=project.name)
    if db_project:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Project with this name already exists"
        )
    
    # Clone or update the repository
    try:
        repo_path = git_service.clone_or_update_repo(project.git_url, project.name)
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Failed to clone repository: {str(e)}"
        )
    
    # Get commit history
    try:
        commits = git_service.get_commit_history(repo_path)
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Failed to get commit history: {str(e)}"
        )
    
    # Create project in database
    project_data = project.model_dump()
    db_project = crud.project.create(project_data)
    
    if db_project is None:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to create project in database"
        )
    
    # Add commits to database
    successful_commits = 0
    for commit_data in commits:
        if isinstance(commit_data["files_changed"], str):
            try:
                if commit_data["files_changed"].startswith('{') and commit_data["files_changed"].endswith('}'):
                    files_str = commit_data["files_changed"][1:-1]
                    commit_data["files_changed"] = [f.strip() for f in files_str.split(',') if f.strip()]
                else:
                    commit_data["files_changed"] = [commit_data["files_changed"]]
            except:
                commit_data["files_changed"] = []
        
        commit_data["project_id"] = db_project["id"]
        result = crud.commit.create_commit(commit_data)
        if result:
            successful_commits += 1
    
    logger.info(f"Created project with {successful_commits}/{len(commits)} commits")
    
    # Index commits in vector database
    try:
        embedding_service.index_commit_history(db_project["id"], commits)
    except Exception as e:
        logger.error(f"Failed to index commits: {e}")
    
    return db_project

@router.get("/", response_model=List[schemas.Project])
def read_projects(skip: int = 0, limit: int = 100):
    """Get all projects"""
    projects = crud.project.get_multi(skip=skip, limit=limit)
    return projects

@router.get("/{project_id}", response_model=schemas.Project)
def read_project(project_id: int):
    """Get a specific project"""
    db_project = crud.project.get(project_id)
    if db_project is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )
    return db_project

@router.get("/{project_id}/commits", response_model=List[schemas.Commit])
def read_project_commits(project_id: int, skip: int = 0, limit: int = 100):
    """Get commits for a specific project"""
    db_project = crud.project.get(project_id)
    if db_project is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )
    
    commits = crud.commit.get_by_project(project_id=project_id, skip=skip, limit=limit)
    return commits

@router.post("/{project_id}/refresh")
def refresh_project(project_id: int):
    """Refresh a project's data from git"""
    db_project = crud.project.get(project_id)
    if db_project is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )
    
    try:
        repo_path = git_service.clone_or_update_repo(db_project["git_url"], db_project["name"])
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Failed to update repository: {str(e)}"
        )
    
    try:
        commits = git_service.get_commit_history(repo_path)
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Failed to get commit history: {str(e)}"
        )
    
    try:
        embedding_service.index_commit_history(db_project["id"], commits)
        return {"message": "Project refreshed successfully"}
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Failed to refresh project: {str(e)}"
        )



--- File: C:\Shashank\SaaS\code-rag-debugger\backend\routers\references.py ---

from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from typing import List, Optional
import logging

from models import schemas
from models import crud
from workers.reference_indexer import reference_indexer
from services.reference_service import reference_service

router = APIRouter(prefix="/references", tags=["references"])
logger = logging.getLogger(__name__)

@router.post("/projects/{project_id}/index")
async def index_project_references(
    project_id: int, 
    background_tasks: BackgroundTasks,
    force_full_reindex: bool = False
):
    """Trigger reference indexing for a project"""
    db_project = crud.project.get(project_id)
    if db_project is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )
    
    # Run indexing in background
    background_tasks.add_task(
        reference_indexer.index_project_symbols,
        project_id,
        None,
        None,
        force_full_reindex
    )
    
    return {"message": "Reference indexing started in background"}

@router.get("/projects/{project_id}/symbols", response_model=List[schemas.Symbol])
def get_project_symbols(
    project_id: int,
    skip: int = 0,
    limit: int = 100,
    symbol_type: Optional[str] = None,
    file_path: Optional[str] = None
):
    """Get symbols for a project with optional filtering"""
    db_project = crud.project.get(project_id)
    if db_project is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )
    
    symbols = crud.symbol.get_by_project(project_id, skip, limit)
    
    # Apply filters
    if symbol_type:
        symbols = [s for s in symbols if s['symbol_type'] == symbol_type]
    if file_path:
        symbols = [s for s in symbols if s['file_path'] == file_path]
    
    return symbols

@router.get("/symbols/{symbol_id}/references", response_model=List[schemas.Reference])
def get_symbol_references(symbol_id: int, max_depth: int = 1, max_references: int = 50):
    """Get references for a specific symbol"""
    symbol = crud.symbol.get(symbol_id)
    if symbol is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Symbol not found"
        )
    
    references = reference_service.get_symbol_references(symbol_id, max_depth, max_references)
    return references

@router.post("/debug/reference-pack")
def get_debug_reference_pack(
    request: schemas.DebugReferenceRequest
) -> schemas.ReferencePack:
    """Get a reference pack for debugging"""
    try:
        # Resolve snippet to symbol
        symbol_id = reference_service.resolve_snippet_to_symbol(
            request.project_id,
            request.file_path,
            request.error_snippet,
            (request.start_line, request.end_line) if request.start_line and request.end_line else None
        )
        
        if not symbol_id:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Could not resolve symbol from the provided snippet"
            )
        
        # Build reference pack
        reference_pack = reference_service.build_reference_pack(
            symbol_id,
            request.token_budget,
            request.ranking_params
        )
        
        return reference_pack
        
    except Exception as e:
        logger.error(f"Failed to build reference pack: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to build reference pack: {str(e)}"
        )

@router.get("/projects/{project_id}/indexing-jobs", response_model=List[schemas.IndexingJob])
def get_indexing_jobs(project_id: int):
    """Get indexing jobs for a project"""
    db_project = crud.project.get(project_id)
    if db_project is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )
    
    jobs = crud.indexing_job.get_by_project(project_id)
    return jobs


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\routers\__init__.py ---

from .projects import router as projects_router
from .debug import router as debug_router
from .history import router as history_router

__all__ = [
    "projects_router",
    "debug_router", 
    "history_router"
]


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\services\embedding_service.py ---

from pinecone import Pinecone
import requests
from typing import List, Dict, Any
from config import settings
import logging
import uuid
from datetime import datetime

logger = logging.getLogger(__name__)

class EmbeddingService:
    def __init__(self):
        # Initialize Pinecone client correctly (v3+)
        try:
            self.pc = Pinecone(api_key=settings.PINECONE_API_KEY)
            logger.info("Pinecone client initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Pinecone client: {e}")
            raise
        
        self.index_name = settings.PINECONE_INDEX_NAME
        # Check if index exists; create if not
        existing_indexes = [idx["name"] for idx in self.pc.list_indexes()]
        if self.index_name not in existing_indexes:
            try:
                self.pc.create_index(
                    name=self.index_name,
                    dimension=384,  # Dimension for all-MiniLM-L6-v2 embeddings
                    metric="cosine"
                )
                logger.info(f"Created Pinecone index '{self.index_name}'")
            except Exception as e:
                logger.error(f"Failed to create Pinecone index: {e}")
                raise
        
        self.index = self.pc.Index(self.index_name)

    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using Gemini API's sentence-transformers endpoint"""
        try:
            headers = {
                "Authorization": f"Bearer {settings.GEMINI_API_KEY}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "inputs": texts,
                "truncate": True
            }
            
            response = requests.post(
                "https://api.gemini.com/v1/inference/sentence-transformers/all-MiniLM-L6-v2",  # Replace with actual Gemini embeddings endpoint
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                logger.error(f"Gemini embeddings error: {response.status_code} - {response.text}")
                # Fallback: return dummy embeddings for testing
                return [[0.1] * 384 for _ in texts]
                
        except Exception as e:
            logger.error(f"Failed to get embeddings: {e}")
            return [[0.1] * 384 for _ in texts]

    def create_collection(self, collection_name: str):
        """In Pinecone v3, use namespaces instead of collections"""
        return collection_name

    def add_documents(self, collection_name: str, documents: List[Dict[str, Any]]):
        """Add documents to Pinecone namespace"""
        try:
            texts = [doc["content"] for doc in documents]
            metadatas = [doc["metadata"] for doc in documents]
            
            embeddings = self.get_embeddings(texts)
            
            vectors = []
            for i, (embedding, metadata) in enumerate(zip(embeddings, metadatas)):
                vectors.append({
                    "id": str(uuid.uuid4()),
                    "values": embedding,
                    "metadata": {
                        **metadata,
                        "text": texts[i],
                        "collection": collection_name,
                        "timestamp": datetime.now().isoformat()
                    }
                })
            
            self.index.upsert(vectors=vectors, namespace=collection_name)
            
            logger.info(f"Added {len(documents)} documents to namespace {collection_name}")
            
        except Exception as e:
            logger.error(f"Failed to add documents to namespace {collection_name}: {e}")
            raise

    def query_collection(self, collection_name: str, query_text: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """Query a namespace for similar documents"""
        try:
            query_embedding = self.get_embeddings([query_text])[0]
            
            results = self.index.query(
                vector=query_embedding,
                top_k=n_results,
                namespace=collection_name,
                include_metadata=True,
                include_values=False
            )
            
            formatted_results = []
            for match in results["matches"]:
                formatted_results.append({
                    "content": match["metadata"].get("text", ""),
                    "metadata": {k: v for k, v in match["metadata"].items() if k != "text"},
                    "distance": match["score"],
                    "id": match["id"]
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Failed to query namespace {collection_name}: {e}")
            return []

    def index_commit_history(self, project_id: int, commits: List[Dict[str, Any]]):
        """Index commit history for a project"""
        collection_name = f"project_{project_id}_commits"
        documents = []
        
        for commit in commits:
            content = (
                f"Commit: {commit['hash']}\n"
                f"Author: {commit['author']}\n"
                f"Message: {commit['message']}\n"
                f"Files: {', '.join(commit['files_changed'])}"
            )
            
            documents.append({
                "content": content,
                "metadata": {
                    "type": "commit",
                    "hash": commit["hash"],
                    "author": commit["author"],
                    "timestamp": commit["timestamp"].isoformat(),
                    "project_id": project_id
                }
            })
        
        self.add_documents(collection_name, documents)

    def index_code_files(self, project_id: int, repo_path: str, file_patterns: List[str] = None):
        """Index code files for a project (to be implemented if needed)"""
        pass

    # Add this method to the EmbeddingService class

def upsert_symbol_embedding(self, namespace: str, chunk_id: int, embedding: List[float], metadata: Dict[str, Any]):
    """Upsert a symbol embedding into Pinecone"""
    try:
        vector = {
            "id": str(chunk_id),
            "values": embedding,
            "metadata": metadata
        }
        
        self.index.upsert(vectors=[vector], namespace=namespace)
        logger.info(f"Upserted symbol embedding {chunk_id} to namespace {namespace}")
        
    except Exception as e:
        logger.error(f"Failed to upsert symbol embedding: {e}")
        raise

embedding_service = EmbeddingService()



--- File: C:\Shashank\SaaS\code-rag-debugger\backend\services\gemini_client.py ---

import requests
import logging
from typing import List, Optional
from config import settings

logger = logging.getLogger(__name__)

class GeminiClient:
    def __init__(self):
        self.api_key = settings.GEMINI_API_KEY
        self.base_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

    def generate(self, prompt: str, context: Optional[List[str]] = None) -> str:
        """Generate a response using Gemini API (Google Generative Language API)"""
        if not self.api_key:
            logger.error("Gemini API key is missing")
            return "API service is currently unavailable"

        try:
            full_prompt = self._build_prompt(prompt, context)

            headers = {
                "Content-Type": "application/json"
            }

            payload = {
                "contents": [
                    {
                        "parts": [
                            {
                                "text": full_prompt
                            }
                        ]
                    }
                ]
            }

            response = requests.post(
                f"{self.base_url}?key={self.api_key}",
                headers=headers,
                json=payload,
                timeout=120
            )

            if response.status_code == 200:
                data = response.json()
                # Gemini's expected response format
                try:
                    return data["candidates"][0]["content"]["parts"][0]["text"].strip()
                except (KeyError, IndexError) as e:
                    logger.error(f"Unexpected Gemini API response format: {data}")
                    return "Unexpected response format from API"
            else:
                logger.error(f"Gemini API error: {response.status_code} - {response.text}")
                return "Sorry, I couldn't process your request at this time."

        except requests.exceptions.RequestException as e:
            logger.error(f"Request to Gemini API failed: {e}")
            return "Connection to the AI service failed. Please try again later."

    def _build_prompt(self, prompt: str, context: Optional[List[str]] = None) -> str:
        """Build a prompt with optional context"""
        if not context:
            return prompt

        context_str = "\n".join([f"Context {i+1}: {ctx}" for i, ctx in enumerate(context)])
        return f"""Based on the following context:

{context_str}

Please respond to this query: {prompt}

Your response should be helpful, concise, and focused on solving the problem.

Response:"""


gemini_client = GeminiClient()



--- File: C:\Shashank\SaaS\code-rag-debugger\backend\services\git_service.py ---

import git
import os
from typing import List, Dict, Any
from datetime import datetime
from config import settings
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class GitService:
    def __init__(self):
        self.repo_base_path = Path(settings.REPOSITORY_BASE_PATH)
        self.repo_base_path.mkdir(exist_ok=True)

    def clone_or_update_repo(self, git_url: str, project_name: str) -> str:
        """Clone or update a git repository"""
        repo_path = self.repo_base_path / project_name
        
        try:
            if repo_path.exists():
                # Update existing repository
                repo = git.Repo(repo_path)
                origin = repo.remotes.origin
                origin.pull()
                logger.info(f"Updated repository: {project_name}")
            else:
                # Clone new repository
                repo = git.Repo.clone_from(git_url, repo_path)
                logger.info(f"Cloned repository: {project_name}")
                
            return str(repo_path)
        except git.exc.GitCommandError as e:
            logger.error(f"Git operation failed for {project_name}: {e}")
            raise Exception(f"Failed to clone or update repository: {e}")

    def get_commit_history(self, repo_path: str, max_commits: int = None) -> List[Dict[str, Any]]:
        """Get commit history from a repository"""
        if max_commits is None:
            max_commits = settings.MAX_COMMIT_HISTORY
            
        try:
            repo = git.Repo(repo_path)
            commits = []
            
            for commit in repo.iter_commits(max_count=max_commits):
                # Get files changed in this commit as a proper list
                files_changed = []
                try:
                    # Get the diff for this commit and extract file paths
                    if commit.parents:
                        # Compare with parent commit
                        parent = commit.parents[0]
                        diff = parent.diff(commit)
                        files_changed = [d.a_path for d in diff if d.a_path]
                    else:
                        # Initial commit - get all files in the tree
                        files_changed = [item.path for item in commit.tree.traverse() if isinstance(item, git.Blob)]
                except (IndexError, AttributeError, Exception) as e:
                    logger.warning(f"Could not get files changed for commit {commit.hexsha}: {e}")
                    # Fallback: try to get files from stats
                    try:
                        files_changed = list(commit.stats.files.keys())
                    except:
                        files_changed = []
                
                commits.append({
                    "hash": commit.hexsha,
                    "author": str(commit.author),
                    "message": commit.message.strip(),
                    "timestamp": datetime.fromtimestamp(commit.committed_date),
                    "files_changed": files_changed  # This should be a list, not a string
                })
                
            return commits
        except git.exc.InvalidGitRepositoryError:
            logger.error(f"Invalid git repository: {repo_path}")
            raise Exception(f"Path is not a valid git repository: {repo_path}")

    def get_file_content(self, repo_path: str, file_path: str, commit_hash: str = None) -> str:
        """Get content of a file at a specific commit"""
        try:
            repo = git.Repo(repo_path)
            
            if commit_hash:
                # Get file content at specific commit
                commit = repo.commit(commit_hash)
                return commit.tree[file_path].data_stream.read().decode('utf-8')
            else:
                # Get current file content
                file_path_obj = Path(repo_path) / file_path
                with open(file_path_obj, 'r', encoding='utf-8') as f:
                    return f.read()
                    
        except (KeyError, FileNotFoundError):
            logger.error(f"File not found: {file_path} at commit {commit_hash}")
            raise Exception(f"File not found: {file_path}")
        except git.exc.BadName:
            logger.error(f"Invalid commit hash: {commit_hash}")
            raise Exception(f"Invalid commit hash: {commit_hash}")

    def get_diff(self, repo_path: str, commit_hash: str) -> str:
        """Get the diff for a specific commit"""
        try:
            repo = git.Repo(repo_path)
            commit = repo.commit(commit_hash)
            
            if commit.parents:
                # Compare with parent commit
                parent = commit.parents[0]
                return repo.git.diff(parent.hexsha, commit.hexsha)
            else:
                # Initial commit - show all files
                return repo.git.show(commit.hexsha)
                
        except git.exc.BadName:
            logger.error(f"Invalid commit hash: {commit_hash}")
            raise Exception(f"Invalid commit hash: {commit_hash}")

git_service = GitService()


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\services\rag_service.py ---

from typing import List, Dict, Any
from .retrieval_service import retrieval_service
from .gemini_client import gemini_client
import logging

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        pass

    def generate_debug_response(self, project_id: int, error_message: str, 
                              code_snippet: str = None, file_path: str = None, 
                              additional_context: str = None,
                              use_reference_pack: bool = True) -> Dict[str, Any]:
        """Generate a debug response using RAG, optionally with reference packs"""
        try:
            if use_reference_pack and code_snippet and file_path:
                # Try to use reference pack for more precise context
                try:
                    from services.reference_service import reference_service
                    
                    # Resolve snippet to symbol and build reference pack
                    symbol_id = reference_service.resolve_snippet_to_symbol(
                        project_id, file_path, code_snippet
                    )
                    
                    if symbol_id:
                        reference_pack = reference_service.build_reference_pack(symbol_id)
                        context = self._build_context_from_reference_pack(
                            reference_pack, error_message, code_snippet, additional_context
                        )
                    else:
                        # Fallback to traditional retrieval
                        context = self._build_context_traditional(
                            project_id, error_message, code_snippet, file_path, additional_context
                        )
                except Exception as e:
                    logger.warning(f"Reference pack failed, falling back: {e}")
                    context = self._build_context_traditional(
                        project_id, error_message, code_snippet, file_path, additional_context
                    )
            else:
                # Use traditional retrieval
                context = self._build_context_traditional(
                    project_id, error_message, code_snippet, file_path, additional_context
                )
            
            # Generate response using Gemini
            prompt = self._build_debug_prompt(error_message, context)
            response = gemini_client.generate(prompt, context)
            
            # Calculate confidence
            confidence = self._calculate_confidence(context, response)
            
            return {
                "solution": response,
                "context": {
                    "similar_errors": context.get('similar_errors', []),
                    "documentation": context.get('documentation', []),
                    "code_suggestions": context.get('code_suggestions', [])
                },
                "confidence": confidence
            }
            
        except Exception as e:
            logger.error(f"Failed to generate debug response: {e}")
            return {
                "solution": "Sorry, I encountered an error while processing your request.",
                "context": {},
                "confidence": 0.0
            }

    def _build_context_from_reference_pack(self, reference_pack, error_message: str,
                                         code_snippet: str = None, additional_context: str = None) -> Dict[str, Any]:
        """Build context from reference pack"""
        context = {}
        
        # Add reference pack information
        context['reference_pack'] = {
            'symbol_name': reference_pack.symbol.symbol_name,
            'symbol_type': reference_pack.symbol.symbol_type,
            'file_path': reference_pack.symbol.file_path,
            'lines': f"{reference_pack.symbol.start_line}-{reference_pack.symbol.end_line}",
            'definition': reference_pack.definition,
            'references': reference_pack.references,
            'callers': reference_pack.callers,
            'callees': reference_pack.callees,
            'imports': reference_pack.imports,
            'tests': reference_pack.tests,
            'historical_fixes': reference_pack.historical_fixes,
            'token_count': reference_pack.token_count,
            'reasoning': reference_pack.reasoning
        }
        
        # Add current error context
        context['error'] = error_message
        if code_snippet:
            context['code_snippet'] = code_snippet
        if additional_context:
            context['additional_context'] = additional_context
        
        return context

    def _build_context_traditional(self, project_id: int, error_message: str,
                                 code_snippet: str = None, file_path: str = None,
                                 additional_context: str = None) -> Dict[str, Any]:
        """Traditional context building (existing implementation)"""
        similar_errors = retrieval_service.retrieve_similar_errors(project_id, error_message)
        documentation = retrieval_service.retrieve_relevant_documentation(project_id, error_message)
        code_suggestions = retrieval_service.get_code_suggestions(project_id, error_message, code_snippet)
        
        context = {
            'similar_errors': similar_errors,
            'documentation': documentation,
            'code_suggestions': code_suggestions
        }
        
        if code_snippet:
            context['code_snippet'] = code_snippet
        if file_path:
            context['file_path'] = file_path
        if additional_context:
            context['additional_context'] = additional_context
            
        return context

    def _build_debug_prompt(self, error_message: str, context: Dict[str, Any]) -> str:
        """Build a prompt for debugging"""
        base_prompt = f"""You are an expert software developer helping to debug an issue.

Error: {error_message}

Please provide a helpful solution to fix this error. Be specific and provide code examples if appropriate.

Consider the following context from the project's history and documentation:
"""
        
        if 'reference_pack' in context:
            # Use reference pack context
            ref_pack = context['reference_pack']
            context_str = f"""
Symbol: {ref_pack['symbol_name']} ({ref_pack['symbol_type']})
File: {ref_pack['file_path']}:{ref_pack['lines']}

Definition:
{ref_pack['definition']}

References found:
{ref_pack['reasoning']}

Additional context:
"""
            if 'code_snippet' in context:
                context_str += f"\nCurrent code snippet:\n{context['code_snippet']}"
            if 'additional_context' in context:
                context_str += f"\nAdditional context: {context['additional_context']}"
                
            return f"{base_prompt}\n{context_str}\n\nSolution:"
            
        else:
            # Use traditional context
            context_parts = []
            
            if 'similar_errors' in context and context['similar_errors']:
                context_parts.append("Similar errors found in project history:")
                for error in context['similar_errors'][:3]:
                    context_parts.append(f"- {error.get('content', '')[:100]}...")
            
            if 'documentation' in context and context['documentation']:
                context_parts.append("Relevant documentation:")
                for doc in context['documentation'][:2]:
                    context_parts.append(f"- {doc.get('content', '')[:100]}...")
            
            if 'code_suggestions' in context and context['code_suggestions']:
                context_parts.append("General code suggestions:")
                for suggestion in context['code_suggestions'][:3]:
                    context_parts.append(f"- {suggestion}")
            
            if 'code_snippet' in context:
                context_parts.append(f"Current code snippet:\n{context['code_snippet']}")
            
            if 'file_path' in context:
                context_parts.append(f"File path: {context['file_path']}")
                
            if 'additional_context' in context:
                context_parts.append(f"Additional context: {context['additional_context']}")
            
            context_str = "\n".join(context_parts)
            return f"{base_prompt}\n{context_str}\n\nSolution:"

    def _calculate_confidence(self, context: Dict[str, Any], response: str) -> float:
        """Calculate confidence score for the response"""
        if 'reference_pack' in context:
            # Higher confidence when using reference packs
            return 0.8
            
        # Traditional confidence calculation
        similar_errors = context.get('similar_errors', [])
        if not similar_errors:
            return 0.3  # Low confidence if no similar errors found
        
        # Calculate average similarity of retrieved errors
        avg_similarity = sum(error.get('distance', 0) for error in similar_errors) / len(similar_errors)
        
        # Adjust based on response quality (simplified)
        quality_indicator = 1.0 if any(keyword in response.lower() for keyword in ["fix", "solution", "try", "change"]) else 0.5
        
        return min(0.9, avg_similarity * quality_indicator)

rag_service = RAGService()


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\services\reference_ranking.py ---

from typing import List, Dict, Any
import numpy as np
import logging
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class ReferenceRanking:
    def __init__(self):
        # Default ranking weights
        self.default_weights = {
            'semantic_similarity': 0.6,
            'proximity': 0.2,
            'recency': 0.1,
            'usage': 0.08,
            'test_boost': 0.02
        }
        
        # Lambda for recency decay
        self.recency_lambda = 0.1  # ~10 day half-life

    def rank_references(self, reference_candidates: List[Dict[str, Any]], 
                       error_embedding: List[float] = None,
                       params: Dict[str, float] = None) -> List[Dict[str, Any]]:
        """Rank reference candidates using multiple factors"""
        if not reference_candidates:
            return []
        
        weights = params or self.default_weights
        
        ranked_candidates = []
        for candidate in reference_candidates:
            score = self._calculate_score(candidate, error_embedding, weights)
            ranked_candidates.append({
                **candidate,
                'ranking_score': score
            })
        
        # Sort by score descending
        ranked_candidates.sort(key=lambda x: x['ranking_score'], reverse=True)
        return ranked_candidates

    def _calculate_score(self, candidate: Dict[str, Any], 
                        error_embedding: List[float], 
                        weights: Dict[str, float]) -> float:
        """Calculate composite ranking score"""
        scores = {
            'semantic_similarity': self._semantic_similarity_score(candidate, error_embedding),
            'proximity': self._proximity_score(candidate),
            'recency': self._recency_score(candidate),
            'usage': self._usage_score(candidate),
            'test_boost': self._test_boost_score(candidate)
        }
        
        # Weighted sum
        total_score = 0.0
        for factor, weight in weights.items():
            total_score += scores[factor] * weight
        
        return total_score

    def _semantic_similarity_score(self, candidate: Dict[str, Any], 
                                 error_embedding: List[float]) -> float:
        """Calculate semantic similarity score"""
        if not error_embedding or 'embedding' not in candidate:
            return 0.5  # Neutral score if no embedding available
        
        try:
            # Cosine similarity between error and candidate embeddings
            candidate_embedding = candidate['embedding']
            if not candidate_embedding or len(candidate_embedding) != len(error_embedding):
                return 0.5
                
            dot_product = np.dot(error_embedding, candidate_embedding)
            norm_a = np.linalg.norm(error_embedding)
            norm_b = np.linalg.norm(candidate_embedding)
            
            if norm_a == 0 or norm_b == 0:
                return 0.0
                
            return float(dot_product / (norm_a * norm_b))
            
        except Exception as e:
            logger.error(f"Failed to calculate semantic similarity: {e}")
            return 0.5

    def _proximity_score(self, candidate: Dict[str, Any]) -> float:
        """Calculate proximity score based on call distance"""
        call_distance = candidate.get('call_distance', 2)  # Default to max distance
        return 1.0 / (1.0 + call_distance)  # Inverse relationship

    def _recency_score(self, candidate: Dict[str, Any]) -> float:
        """Calculate recency score based on last modification"""
        last_modified = candidate.get('last_modified')
        if not last_modified:
            return 0.5
            
        try:
            if isinstance(last_modified, str):
                last_modified = datetime.fromisoformat(last_modified.replace('Z', '+00:00'))
            
            days_since_modification = (datetime.now() - last_modified).days
            return float(np.exp(-self.recency_lambda * days_since_modification))
            
        except Exception as e:
            logger.error(f"Failed to calculate recency score: {e}")
            return 0.5

    def _usage_score(self, candidate: Dict[str, Any]) -> float:
        """Calculate usage score based on usage count"""
        usage_count = candidate.get('usage_count', 0)
        return float(np.log(1 + usage_count) / 10.0)  # Log scaling, capped

    def _test_boost_score(self, candidate: Dict[str, Any]) -> float:
        """Calculate test boost score if candidate is a test"""
        file_path = candidate.get('file_path', '')
        is_test = any(test_indicator in file_path.lower() 
                     for test_indicator in ['test', 'spec', 'testing'])
        return 1.2 if is_test else 1.0

    def update_weights_from_feedback(self, positive_examples: List[Dict[str, Any]],
                                   negative_examples: List[Dict[str, Any]]):
        """Update ranking weights based on user feedback"""
        # Simple reinforcement learning approach
        # This would be implemented based on collected feedback data
        pass

# Global ranking instance
reference_ranking = ReferenceRanking()


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\services\reference_service.py ---

from typing import List, Dict, Any, Optional
import logging
from models import crud
from models.symbol import Symbol, Reference, ReferencePack
from utils.ast_parsers import ast_parser
from utils.lsp_client import lsp_client
from services.embedding_service import embedding_service
from services.retrieval_service import retrieval_service
import re

logger = logging.getLogger(__name__)

class ReferenceService:
    def __init__(self):
        pass

    def find_enclosing_symbol(self, project_id: int, file_path: str, 
                            start_line: Optional[int] = None, 
                            end_line: Optional[int] = None,
                            snippet_text: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """Find the symbol that encloses the given line range or snippet"""
        try:
            # Try to get from database first
            if start_line is not None:
                symbols = crud.symbol.get_by_file_and_line(project_id, file_path, start_line, end_line)
                if symbols:
                    return symbols[0]
            
            # If not found in DB or no line info, use AST parsing
            repo_path = self._get_repo_path(project_id)
            if not repo_path:
                return None
                
            file_content = self._read_file_content(repo_path, file_path)
            if not file_content:
                return None
                
            # Parse symbols from file
            language = self._detect_language(file_path)
            symbols = self._parse_symbols(language, file_content, file_path)
            
            if start_line is not None:
                # Find by line range
                return ast_parser.find_enclosing_symbol_by_line(symbols, start_line)
            elif snippet_text:
                # Find by semantic similarity
                return self.semantic_find_symbols(project_id, snippet_text, file_path, top_k=1)[0]
                
        except Exception as e:
            logger.error(f"Failed to find enclosing symbol: {e}")
            return None

    def semantic_find_symbols(self, project_id: int, query_snippet: str, 
                             file_path: Optional[str] = None, top_k: int = 10) -> List[Dict[str, Any]]:
        """Find symbols semantically similar to the query snippet"""
        try:
            # Query Pinecone for similar symbols
            namespace = f"project_{project_id}_symbols"
            results = embedding_service.query_collection(namespace, query_snippet, top_k)
            
            symbols = []
            for result in results:
                symbol_id = result['metadata'].get('symbol_id')
                if symbol_id:
                    symbol = crud.symbol.get(symbol_id)
                    if symbol:
                        symbols.append({
                            'symbol': symbol,
                            'similarity': 1 - result['distance'],
                            'metadata': result['metadata']
                        })
            
            # Filter by file path if specified
            if file_path:
                symbols = [s for s in symbols if s['symbol']['file_path'] == file_path]
                
            return sorted(symbols, key=lambda x: x['similarity'], reverse=True)[:top_k]
            
        except Exception as e:
            logger.error(f"Failed to semantically find symbols: {e}")
            return []

    def get_symbol_references(self, symbol_id: int, max_depth: int = 1, 
                            max_references: int = 50) -> List[Dict[str, Any]]:
        """Get references for a symbol with graph traversal"""
        try:
            symbol = crud.symbol.get(symbol_id)
            if not symbol:
                return []
                
            references = []
            visited = set()
            
            def traverse(current_symbol_id, depth):
                if depth > max_depth or len(references) >= max_references:
                    return
                    
                if current_symbol_id in visited:
                    return
                    
                visited.add(current_symbol_id)
                
                # Get direct references
                direct_refs = crud.reference.get_by_symbol(current_symbol_id)
                for ref in direct_refs:
                    references.append({
                        'reference': ref,
                        'depth': depth,
                        'symbol': crud.symbol.get(ref['to_symbol_id'])
                    })
                    
                    # Recursively traverse if within depth limit
                    if depth < max_depth:
                        traverse(ref['to_symbol_id'], depth + 1)
            
            traverse(symbol_id, 0)
            return references
            
        except Exception as e:
            logger.error(f"Failed to get symbol references: {e}")
            return []

    def build_reference_pack(self, symbol_id: int, token_budget: int = 4000,
                           ranking_params: Optional[Dict[str, float]] = None) -> ReferencePack:
        """Build a compact context package for debugging"""
        try:
            symbol = crud.symbol.get(symbol_id)
            if not symbol:
                raise ValueError(f"Symbol {symbol_id} not found")
                
            # Get symbol definition
            definition = self._get_symbol_definition(symbol)
            
            # Get references
            references = self.get_symbol_references(symbol_id, max_depth=2)
            
            # Get historical fixes
            historical_fixes = self._get_historical_fixes(symbol['project_id'], symbol['symbol_name'])
            
            # Rank and select snippets within token budget
            selected_snippets = self._rank_and_select_snippets(
                definition, references, historical_fixes, token_budget, ranking_params
            )
            
            return ReferencePack(
                symbol=symbol,
                definition=selected_snippets['definition'],
                references=selected_snippets['references'],
                callers=selected_snippets['callers'],
                callees=selected_snippets['callees'],
                imports=selected_snippets['imports'],
                tests=selected_snippets['tests'],
                historical_fixes=selected_snippets['historical_fixes'],
                token_count=selected_snippets['total_tokens'],
                reasoning=selected_snippets['reasoning']
            )
            
        except Exception as e:
            logger.error(f"Failed to build reference pack: {e}")
            raise

    def resolve_snippet_to_symbol(self, project_id: int, file_path: str, 
                                snippet_text: str, line_range: Optional[tuple] = None) -> Optional[int]:
        """Resolve a code snippet to a specific symbol"""
        try:
            # First try line-based resolution
            if line_range:
                start_line, end_line = line_range
                symbol = self.find_enclosing_symbol(project_id, file_path, start_line, end_line)
                if symbol:
                    return symbol['id']
            
            # Fallback to semantic search
            results = self.semantic_find_symbols(project_id, snippet_text, file_path, top_k=1)
            if results:
                return results[0]['symbol']['id']
                
            return None
            
        except Exception as e:
            logger.error(f"Failed to resolve snippet to symbol: {e}")
            return None

    def _get_symbol_definition(self, symbol: Dict[str, Any]) -> Dict[str, Any]:
        """Get the full definition of a symbol"""
        repo_path = self._get_repo_path(symbol['project_id'])
        file_content = self._read_file_content(repo_path, symbol['file_path'])
        
        if file_content and symbol['start_line'] and symbol['end_line']:
            lines = file_content.split('\n')
            definition = '\n'.join(lines[symbol['start_line']-1:symbol['end_line']])
            
            return {
                'content': definition,
                'file_path': symbol['file_path'],
                'start_line': symbol['start_line'],
                'end_line': symbol['end_line'],
                'token_count': self._estimate_tokens(definition)
            }
        
        return {'content': '', 'token_count': 0}

    def _get_historical_fixes(self, project_id: int, symbol_name: str) -> List[Dict[str, Any]]:
        """Get historical fixes related to this symbol"""
        # Use retrieval service to find similar errors/fixes
        error_query = f"fix {symbol_name} error bug"
        similar_errors = retrieval_service.retrieve_similar_errors(project_id, error_query, n_results=5)
        
        fixes = []
        for error in similar_errors:
            fixes.append({
                'commit_hash': error['metadata'].get('hash'),
                'message': error['content'],
                'similarity': error.get('similarity', 0.0)
            })
        
        return fixes

    def _rank_and_select_snippets(self, definition: Dict[str, Any], references: List[Dict[str, Any]],
                                historical_fixes: List[Dict[str, Any]], token_budget: int,
                                ranking_params: Optional[Dict[str, float]]) -> Dict[str, Any]:
        """Rank and select snippets within token budget"""
        # Implementation of ranking algorithm
        # This would use the scoring formula mentioned in the architecture
        selected = {
            'definition': definition,
            'references': [],
            'callers': [],
            'callees': [],
            'imports': [],
            'tests': [],
            'historical_fixes': [],
            'total_tokens': definition['token_count'],
            'reasoning': 'Prioritized definition and direct references'
        }
        
        # Simple greedy selection for now
        # TODO: Implement proper ranking algorithm
        for ref in references:
            ref_content = self._get_reference_content(ref)
            token_count = self._estimate_tokens(ref_content)
            
            if selected['total_tokens'] + token_count <= token_budget:
                selected['references'].append({
                    'content': ref_content,
                    'reference_type': ref['reference']['reference_type'],
                    'token_count': token_count
                })
                selected['total_tokens'] += token_count
        
        return selected

    def _get_reference_content(self, reference: Dict[str, Any]) -> str:
        """Get content for a reference"""
        repo_path = self._get_repo_path(reference['symbol']['project_id'])
        file_content = self._read_file_content(repo_path, reference['reference']['file_path'])
        
        if file_content:
            lines = file_content.split('\n')
            # Get context around the reference line
            start_line = max(0, reference['reference']['line'] - 5)
            end_line = min(len(lines), reference['reference']['line'] + 5)
            return '\n'.join(lines[start_line:end_line])
        
        return ""

    def _estimate_tokens(self, text: str) -> int:
        """Estimate token count using simple approximation"""
        return len(text.split())  # 1 token â‰ˆ 1 word

    def _get_repo_path(self, project_id: int) -> Optional[str]:
        """Get repository path for a project"""
        project = crud.project.get(project_id)
        if project:
            return f"./repositories/{project['name']}"
        return None

    def _read_file_content(self, repo_path: str, file_path: str) -> Optional[str]:
        """Read file content from repository"""
        try:
            full_path = f"{repo_path}/{file_path}"
            with open(full_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error(f"Failed to read file {file_path}: {e}")
            return None

    def _detect_language(self, file_path: str) -> str:
        """Detect programming language from file extension"""
        ext_map = {
            '.py': 'python',
            '.js': 'javascript', '.jsx': 'javascript',
            '.ts': 'typescript', '.tsx': 'typescript',
            '.java': 'java',
            '.cpp': 'cpp', '.cc': 'cpp', '.cxx': 'cpp', '.h': 'cpp', '.hpp': 'cpp',
            '.go': 'go',
            '.rs': 'rust'
        }
        
        ext = '.' + file_path.split('.')[-1].lower()
        return ext_map.get(ext, 'unknown')

    def _parse_symbols(self, language: str, file_content: str, file_path: str) -> List[Dict[str, Any]]:
        """Parse symbols from file content using appropriate parser"""
        if language == 'python':
            return ast_parser.parse_python_symbols(file_content, file_path)
        elif language in ['javascript', 'typescript']:
            return ast_parser.parse_javascript_symbols(file_content, file_path)
        elif language == 'java':
            return ast_parser.parse_java_symbols(file_content, file_path)
        elif language == 'cpp':
            return ast_parser.parse_cpp_symbols(file_content, file_path)
        elif language == 'go':
            return ast_parser.parse_go_symbols(file_content, file_path)
        elif language == 'rust':
            return ast_parser.parse_rust_symbols(file_content, file_path)
        else:
            return []

# Global reference service instance
reference_service = ReferenceService()


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\services\retrieval_service.py ---

from typing import List, Dict, Any
from .embedding_service import embedding_service
from .git_service import git_service
import logging

logger = logging.getLogger(__name__)

class RetrievalService:
    def __init__(self):
        pass

    def retrieve_similar_errors(self, project_id: int, error_message: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """Retrieve similar errors from the project's history"""
        try:
            # Query the commit collection for similar errors
            collection_name = f"project_{project_id}_commits"
            results = embedding_service.query_collection(
                collection_name, 
                error_message, 
                n_results
            )
            
            # Filter for commits that might contain error fixes
            error_commits = []
            for result in results:
                if any(keyword in result["content"].lower() for keyword in ["fix", "error", "bug", "issue"]):
                    error_commits.append({
                        "commit": result["metadata"],
                        "similarity": 1 - result["distance"],  # Convert distance to similarity
                        "content": result["content"]
                    })
            
            return error_commits
            
        except Exception as e:
            logger.error(f"Failed to retrieve similar errors: {e}")
            return []

    def retrieve_relevant_documentation(self, project_id: int, query: str, n_results: int = 3) -> List[Dict[str, Any]]:
        """Retrieve relevant documentation for a query"""
        # This would query a documentation collection
        # For now, return empty list as placeholder
        return []

    def get_commit_context(self, project_id: int, commit_hash: str) -> Dict[str, Any]:
        """Get detailed context for a specific commit"""
        try:
            # Get project to find repo path
            # This would require database access which we'll handle in the router
            # For now, return basic info
            return {
                "hash": commit_hash,
                "details": f"Commit {commit_hash} details would be retrieved here"
            }
        except Exception as e:
            logger.error(f"Failed to get commit context: {e}")
            return {}

    def get_code_suggestions(self, project_id: int, error_message: str, code_snippet: str = None) -> List[str]:
        """Get code suggestions based on error and context"""
        # This would use more advanced retrieval and potentially pattern matching
        # For now, return placeholder
        return [
            "Check for null references in your code",
            "Verify input parameters are valid",
            "Ensure all resources are properly disposed"
        ]

retrieval_service = RetrievalService()


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\services\__init__.py ---

from .git_service import git_service
from .embedding_service import embedding_service
from .retrieval_service import retrieval_service
from .rag_service import rag_service
from .gemini_client import gemini_client

__all__ = [
    "git_service",
    "embedding_service",
    "retrieval_service", 
    "rag_service",
    "gemini_client"
]


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\utils\ast_parsers.py ---

import tree_sitter
from tree_sitter import Language, Parser
from typing import List, Dict, Any, Optional
import os
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

# Tree-sitter language builds
TREE_SITTER_LANGUAGES = {
    'python': 'tree_sitter_languages/build/python.so',
    'javascript': 'tree_sitter_languages/build/javascript.so',
    'typescript': 'tree_sitter_languages/build/typescript.so',
    'java': 'tree_sitter_languages/build/java.so',
    'cpp': 'tree_sitter_languages/build/cpp.so',
    'go': 'tree_sitter_languages/build/go.so',
    'rust': 'tree_sitter_languages/build/rust.so'
}

class ASTParser:
    def __init__(self):
        self.parsers = {}
        self._load_languages()

    def _load_languages(self):
        """Load tree-sitter language parsers"""
        for lang, lib_path in TREE_SITTER_LANGUAGES.items():
            try:
                if os.path.exists(lib_path):
                    language = Language(lib_path, lang)
                    parser = Parser()
                    parser.set_language(language)
                    self.parsers[lang] = parser
                    logger.info(f"Loaded parser for {lang}")
                else:
                    logger.warning(f"Tree-sitter library not found for {lang}: {lib_path}")
            except Exception as e:
                logger.error(f"Failed to load parser for {lang}: {e}")

    def parse_python_symbols(self, file_content: str, file_path: str) -> List[Dict[str, Any]]:
        """Parse Python file and extract symbols"""
        return self._parse_symbols('python', file_content, file_path)

    def parse_javascript_symbols(self, file_content: str, file_path: str) -> List[Dict[str, Any]]:
        """Parse JavaScript file and extract symbols"""
        return self._parse_symbols('javascript', file_content, file_path)

    def parse_typescript_symbols(self, file_content: str, file_path: str) -> List[Dict[str, Any]]:
        """Parse TypeScript file and extract symbols"""
        return self._parse_symbols('typescript', file_content, file_path)

    def parse_java_symbols(self, file_content: str, file_path: str) -> List[Dict[str, Any]]:
        """Parse Java file and extract symbols"""
        return self._parse_symbols('java', file_content, file_path)

    def parse_cpp_symbols(self, file_content: str, file_path: str) -> List[Dict[str, Any]]:
        """Parse C++ file and extract symbols"""
        return self._parse_symbols('cpp', file_content, file_path)

    def parse_go_symbols(self, file_content: str, file_path: str) -> List[Dict[str, Any]]:
        """Parse Go file and extract symbols"""
        return self._parse_symbols('go', file_content, file_path)

    def parse_rust_symbols(self, file_content: str, file_path: str) -> List[Dict[str, Any]]:
        """Parse Rust file and extract symbols"""
        return self._parse_symbols('rust', file_content, file_path)

    def _parse_symbols(self, language: str, file_content: str, file_path: str) -> List[Dict[str, Any]]:
        """Generic symbol parsing method"""
        if language not in self.parsers:
            logger.warning(f"No parser available for {language}")
            return []

        try:
            parser = self.parsers[language]
            tree = parser.parse(bytes(file_content, 'utf8'))
            root_node = tree.root_node

            symbols = []
            
            # Language-specific extraction logic
            if language == 'python':
                symbols = self._extract_python_symbols(root_node, file_content)
            elif language in ['javascript', 'typescript']:
                symbols = self._extract_javascript_symbols(root_node, file_content)
            elif language == 'java':
                symbols = self._extract_java_symbols(root_node, file_content)
            elif language == 'cpp':
                symbols = self._extract_cpp_symbols(root_node, file_content)
            elif language == 'go':
                symbols = self._extract_go_symbols(root_node, file_content)
            elif language == 'rust':
                symbols = self._extract_rust_symbols(root_node, file_content)

            # Add file path and language to each symbol
            for symbol in symbols:
                symbol['file_path'] = file_path
                symbol['language'] = language

            return symbols

        except Exception as e:
            logger.error(f"Failed to parse {language} file {file_path}: {e}")
            return []

    def _extract_python_symbols(self, root_node, file_content: str) -> List[Dict[str, Any]]:
        """Extract symbols from Python code"""
        symbols = []
        
        def traverse(node):
            if node.type in ['function_definition', 'class_definition']:
                # Get symbol name
                name_node = node.child_by_field_name('name')
                if name_node:
                    symbol_name = file_content[name_node.start_byte:name_node.end_byte].decode('utf8')
                    
                    # Get signature/docstring
                    signature = self._get_python_signature(node, file_content)
                    docstring = self._get_python_docstring(node, file_content)
                    
                    symbols.append({
                        'symbol_name': symbol_name,
                        'symbol_type': 'function' if node.type == 'function_definition' else 'class',
                        'start_line': node.start_point[0] + 1,
                        'end_line': node.end_point[0] + 1,
                        'signature': signature,
                        'docstring': docstring,
                        'code_snippet': file_content[node.start_byte:node.end_byte].decode('utf8')
                    })
            
            # Traverse children
            for child in node.children:
                traverse(child)
        
        traverse(root_node)
        return symbols

    def _get_python_signature(self, node, file_content: str) -> str:
        """Extract function/class signature"""
        if node.type == 'function_definition':
            # Get parameters
            parameters_node = node.child_by_field_name('parameters')
            if parameters_node:
                return file_content[parameters_node.start_byte:parameters_node.end_byte].decode('utf8')
        return ""

    def _get_python_docstring(self, node, file_content: str) -> str:
        """Extract docstring from Python node"""
        # Look for string literal immediately after definition
        for child in node.children:
            if child.type == 'expression_statement':
                string_node = child.child_by_field_name('value')
                if string_node and string_node.type == 'string':
                    return file_content[string_node.start_byte:string_node.end_byte].decode('utf8')
        return ""

    def _extract_javascript_symbols(self, root_node, file_content: str) -> List[Dict[str, Any]]:
        """Extract symbols from JavaScript/TypeScript code"""
        symbols = []
        # Implementation for JS/TS symbol extraction
        # This is a simplified version - would need more complex logic
        def traverse(node):
            if node.type in ['function_declaration', 'class_declaration', 'variable_declarator']:
                name_node = node.child_by_field_name('name')
                if name_node:
                    symbol_name = file_content[name_node.start_byte:name_node.end_byte].decode('utf8')
                    symbol_type = self._get_js_symbol_type(node.type)
                    
                    symbols.append({
                        'symbol_name': symbol_name,
                        'symbol_type': symbol_type,
                        'start_line': node.start_point[0] + 1,
                        'end_line': node.end_point[0] + 1,
                        'code_snippet': file_content[node.start_byte:node.end_byte].decode('utf8')
                    })
            
            for child in node.children:
                traverse(child)
        
        traverse(root_node)
        return symbols

    def _get_js_symbol_type(self, node_type: str) -> str:
        """Map JS node type to symbol type"""
        type_map = {
            'function_declaration': 'function',
            'class_declaration': 'class',
            'variable_declarator': 'variable'
        }
        return type_map.get(node_type, 'variable')

    # Similar methods for other languages would be implemented here
    def _extract_java_symbols(self, root_node, file_content: str) -> List[Dict[str, Any]]:
        """Extract symbols from Java code"""
        return []  # Placeholder

    def _extract_cpp_symbols(self, root_node, file_content: str) -> List[Dict[str, Any]]:
        """Extract symbols from C++ code"""
        return []  # Placeholder

    def _extract_go_symbols(self, root_node, file_content: str) -> List[Dict[str, Any]]:
        """Extract symbols from Go code"""
        return []  # Placeholder

    def _extract_rust_symbols(self, root_node, file_content: str) -> List[Dict[str, Any]]:
        """Extract symbols from Rust code"""
        return []  # Placeholder

    def find_enclosing_symbol_by_line(self, symbols: List[Dict[str, Any]], line: int) -> Optional[Dict[str, Any]]:
        """Find the symbol that encloses the given line number"""
        for symbol in symbols:
            if symbol['start_line'] <= line <= symbol['end_line']:
                return symbol
        return None

# Global parser instance
ast_parser = ASTParser()


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\utils\chunking.py ---

from typing import List, Dict, Any
import re

def chunk_commits(commits: List[Dict[str, Any]], max_chunk_size: int = 1000) -> List[Dict[str, Any]]:
    """Chunk commits into smaller pieces for embedding"""
    chunks = []
    
    for commit in commits:
        content = f"Commit: {commit['hash']}\nAuthor: {commit['author']}\nMessage: {commit['message']}\nFiles: {', '.join(commit['files_changed'])}"
        
        # If content is too long, split it
        if len(content) > max_chunk_size:
            # Split by lines and create smaller chunks
            lines = content.split('\n')
            current_chunk = []
            current_size = 0
            
            for line in lines:
                if current_size + len(line) + 1 > max_chunk_size and current_chunk:
                    chunks.append({
                        "content": '\n'.join(current_chunk),
                        "metadata": {
                            "type": "commit",
                            "hash": commit["hash"],
                            "author": commit["author"],
                            "timestamp": commit["timestamp"].isoformat(),
                            "chunk_type": "partial"
                        }
                    })
                    current_chunk = []
                    current_size = 0
                
                current_chunk.append(line)
                current_size += len(line) + 1
            
            if current_chunk:
                chunks.append({
                    "content": '\n'.join(current_chunk),
                    "metadata": {
                        "type": "commit",
                        "hash": commit["hash"],
                        "author": commit["author"],
                        "timestamp": commit["timestamp"].isoformat(),
                        "chunk_type": "partial"
                    }
                })
        else:
            chunks.append({
                "content": content,
                "metadata": {
                    "type": "commit",
                    "hash": commit["hash"],
                    "author": commit["author"],
                    "timestamp": commit["timestamp"].isoformat(),
                    "chunk_type": "full"
                }
            })
    
    return chunks

def chunk_text(text: str, metadata: Dict[str, Any], max_chunk_size: int = 1000) -> List[Dict[str, Any]]:
    """Chunk text into smaller pieces for embedding"""
    chunks = []
    
    # Split by paragraphs or sentences
    paragraphs = re.split(r'\n\s*\n', text)
    
    current_chunk = []
    current_size = 0
    
    for paragraph in paragraphs:
        if current_size + len(paragraph) + 2 > max_chunk_size and current_chunk:
            chunks.append({
                "content": '\n\n'.join(current_chunk),
                "metadata": {**metadata, "chunk_type": "partial"}
            })
            current_chunk = []
            current_size = 0
        
        current_chunk.append(paragraph)
        current_size += len(paragraph) + 2
    
    if current_chunk:
        chunks.append({
            "content": '\n\n'.join(current_chunk),
            "metadata": {**metadata, "chunk_type": "full"}
        })
    
    return chunks


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\utils\file_processing.py ---

import os
from pathlib import Path
from typing import List, Dict, Any
import logging
import re

logger = logging.getLogger(__name__)

def find_code_files(directory: str, extensions: List[str] = None) -> List[str]:
    """
    Find all code files in a directory with the given extensions.
    
    Args:
        directory: The directory to search in
        extensions: List of file extensions to include (e.g., ['.py', '.js', '.java'])
    
    Returns:
        List of file paths relative to the directory
    """
    if extensions is None:
        extensions = ['.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.hpp', '.go', '.rs', '.rb', '.php']
    
    code_files = []
    directory_path = Path(directory)
    
    try:
        for extension in extensions:
            for file_path in directory_path.rglob(f"*{extension}"):
                if file_path.is_file():
                    # Get relative path
                    relative_path = file_path.relative_to(directory_path)
                    code_files.append(str(relative_path))
    except Exception as e:
        logger.error(f"Error finding code files in {directory}: {e}")
    
    return code_files

def read_code_files(directory: str, file_paths: List[str]) -> Dict[str, str]:
    """
    Read the content of multiple code files.
    
    Args:
        directory: The base directory
        file_paths: List of relative file paths
    
    Returns:
        Dictionary mapping file paths to their content
    """
    contents = {}
    directory_path = Path(directory)
    
    for file_path in file_paths:
        try:
            full_path = directory_path / file_path
            if full_path.exists() and full_path.is_file():
                with open(full_path, 'r', encoding='utf-8') as f:
                    contents[str(file_path)] = f.read()
        except UnicodeDecodeError:
            logger.warning(f"Could not read file {file_path} (encoding issue)")
        except Exception as e:
            logger.error(f"Error reading file {file_path}: {e}")
    
    return contents

def extract_code_metadata(file_path: str, content: str) -> Dict[str, Any]:
    """
    Extract metadata from code file content.
    
    Args:
        file_path: The file path
        content: The file content
    
    Returns:
        Dictionary with extracted metadata
    """
    metadata = {
        "file_path": file_path,
        "file_extension": os.path.splitext(file_path)[1],
        "lines_of_code": len(content.splitlines()),
        "size_bytes": len(content.encode('utf-8')),
        "imports": [],
        "functions": [],
        "classes": []
    }
    
    # Extract imports (simple regex-based approach)
    import_patterns = {
        '.py': r'^(?:from\s+(\S+)\s+)?import\s+([^\n#]+)',
        '.js': r'^(?:import\s+[^\']+from\s+)?[\'"]([^\'"]+)[\'"]',  # Fixed: escaped single quote
        '.java': r'^import\s+([^;]+);'
    }
    
    extension = metadata["file_extension"]
    if extension in import_patterns:
        matches = re.findall(import_patterns[extension], content, re.MULTILINE)
        if matches:
            if extension == '.py':
                # For Python, matches are tuples (from_module, import_target)
                metadata["imports"] = [f"{frm} {imp}" if frm else imp for frm, imp in matches if imp]
            else:
                metadata["imports"] = [match[0] if isinstance(match, tuple) else match for match in matches]
    
    # Extract function definitions
    function_patterns = {
        '.py': r'^def\s+(\w+)\s*\([^)]*\)\s*:',
        '.js': r'^(?:function\s+(\w+)\s*\([^)]*\)|const\s+(\w+)\s*=\s*\([^)]*\)\s*=>|let\s+(\w+)\s*=\s*\([^)]*\)\s*=>)',
        '.java': r'^(?:public|private|protected)\s+[^{]+\s+(\w+)\s*\([^)]*\)\s*\{'
    }
    
    if extension in function_patterns:
        matches = re.findall(function_patterns[extension], content, re.MULTILINE)
        if matches:
            # Flatten tuples and filter empty strings
            flat_matches = []
            for match in matches:
                if isinstance(match, tuple):
                    flat_matches.extend([m for m in match if m])
                elif match:
                    flat_matches.append(match)
            metadata["functions"] = flat_matches
    
    # Extract class definitions
    class_patterns = {
        '.py': r'^class\s+(\w+)',
        '.js': r'^class\s+(\w+)',
        '.java': r'^class\s+(\w+)'
    }
    
    if extension in class_patterns:
        matches = re.findall(class_patterns[extension], content, re.MULTILINE)
        metadata["classes"] = matches
    
    return metadata

def get_file_tree(directory: str, max_depth: int = 3) -> Dict[str, Any]:
    """
    Generate a file tree structure for the directory.
    
    Args:
        directory: The directory to scan
        max_depth: Maximum depth to traverse
    
    Returns:
        Nested dictionary representing the file tree
    """
    def build_tree(path: Path, current_depth: int = 0) -> Dict[str, Any]:
        if current_depth > max_depth:
            return {}
        
        if path.is_file():
            return {
                "name": path.name,
                "type": "file",
                "size": path.stat().st_size,
                "modified": path.stat().st_mtime
            }
        elif path.is_dir():
            tree = {
                "name": path.name,
                "type": "directory",
                "children": {}
            }
            try:
                for item in path.iterdir():
                    if item.name.startswith('.'):
                        continue  # Skip hidden files/directories
                    tree["children"][item.name] = build_tree(item, current_depth + 1)
            except PermissionError:
                logger.warning(f"Permission denied accessing {path}")
            return tree
        return {}
    
    return build_tree(Path(directory))


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\utils\logging.py ---

import logging
import sys
from typing import Optional
from pathlib import Path
import json
from datetime import datetime

def setup_logging(
    log_level: str = "INFO",
    log_file: Optional[str] = None,
    console_output: bool = True
) -> None:
    """
    Set up logging configuration.
    
    Args:
        log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Path to log file (optional)
        console_output: Whether to output logs to console
    """
    # Convert string level to logging level
    level = getattr(logging, log_level.upper(), logging.INFO)
    
    # Clear any existing handlers
    logging.getLogger().handlers.clear()
    
    # Create formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Create console handler if requested
    if console_output:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        console_handler.setLevel(level)
        logging.getLogger().addHandler(console_handler)
    
    # Create file handler if log file is specified
    if log_file:
        # Create directory if it doesn't exist
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        file_handler.setLevel(level)
        logging.getLogger().addHandler(file_handler)
    
    # Set the root logger level
    logging.getLogger().setLevel(level)
    
    # Set levels for specific noisy loggers
    logging.getLogger("chromadb").setLevel(logging.WARNING)
    logging.getLogger("git").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    
    logging.info(f"Logging setup complete. Level: {log_level}")

def get_logger(name: str) -> logging.Logger:
    """
    Get a logger with the given name.
    
    Args:
        name: Logger name
    
    Returns:
        Configured logger instance
    """
    return logging.getLogger(name)

class JsonFormatter(logging.Formatter):
    """Custom formatter that outputs logs in JSON format."""
    
    def format(self, record: logging.LogRecord) -> str:
        log_data = {
            "timestamp": datetime.fromtimestamp(record.created).isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        
        # Add exception info if present
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        
        return json.dumps(log_data)

def setup_json_logging(
    log_file: str,
    log_level: str = "INFO"
) -> None:
    """
    Set up JSON logging to a file.
    
    Args:
        log_file: Path to JSON log file
        log_level: Logging level
    """
    level = getattr(logging, log_level.upper(), logging.INFO)
    
    # Create directory if it doesn't exist
    log_path = Path(log_file)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Create JSON file handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(JsonFormatter())
    file_handler.setLevel(level)
    
    # Add to root logger
    logging.getLogger().addHandler(file_handler)
    logging.getLogger().setLevel(level)


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\utils\lsp_client.py ---

import subprocess
import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

class LSPClient:
    def __init__(self):
        self.servers = {}
        self.processes = {}

    def start_lsp_server(self, language: str, project_root: str) -> bool:
        """Start an LSP server for the given language"""
        servers = {
            'python': ['pylsp'],
            'javascript': ['typescript-language-server', '--stdio'],
            'typescript': ['typescript-language-server', '--stdio'],
            'java': ['java', '-jar', 'path/to/eclipse.jdt.ls.jar'],
            'cpp': ['clangd'],
            'go': ['gopls'],
            'rust': ['rust-analyzer']
        }
        
        if language not in servers:
            logger.warning(f"No LSP server configured for {language}")
            return False
        
        try:
            cmd = servers[language]
            process = subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=project_root,
                text=True,
                bufsize=1
            )
            
            self.processes[language] = process
            self.servers[language] = {
                'project_root': project_root,
                'initialized': False
            }
            
            # Initialize LSP server
            self._initialize_server(language)
            return True
            
        except Exception as e:
            logger.error(f"Failed to start LSP server for {language}: {e}")
            return False

    def _initialize_server(self, language: str):
        """Initialize LSP server with handshake"""
        if language not in self.processes:
            return
        
        try:
            init_msg = {
                "jsonrpc": "2.0",
                "id": 1,
                "method": "initialize",
                "params": {
                    "processId": self.processes[language].pid,
                    "rootUri": f"file://{self.servers[language]['project_root']}",
                    "capabilities": {}
                }
            }
            
            self._send_message(language, init_msg)
            response = self._receive_message(language)
            
            if response and 'result' in response:
                self.servers[language]['initialized'] = True
                logger.info(f"LSP server for {language} initialized successfully")
                
                # Send initialized notification
                initialized_msg = {
                    "jsonrpc": "2.0",
                    "method": "initialized",
                    "params": {}
                }
                self._send_message(language, initialized_msg)
                
        except Exception as e:
            logger.error(f"Failed to initialize LSP server for {language}: {e}")

    def find_references_via_lsp(self, project_root: str, file_path: str, line: int, column: int) -> List[Dict[str, Any]]:
        """Find references using LSP"""
        language = self._detect_language(file_path)
        if not language or language not in self.processes:
            logger.warning(f"No LSP server available for {file_path}")
            return []
        
        if not self.servers[language]['initialized']:
            if not self.start_lsp_server(language, project_root):
                return []
        
        try:
            # Open the document first
            with open(Path(project_root) / file_path, 'r') as f:
                content = f.read()
            
            open_msg = {
                "jsonrpc": "2.0",
                "method": "textDocument/didOpen",
                "params": {
                    "textDocument": {
                        "uri": f"file://{Path(project_root) / file_path}",
                        "languageId": language,
                        "version": 1,
                        "text": content
                    }
                }
            }
            self._send_message(language, open_msg)
            
            # Find references
            ref_msg = {
                "jsonrpc": "2.0",
                "id": 2,
                "method": "textDocument/references",
                "params": {
                    "textDocument": {
                        "uri": f"file://{Path(project_root) / file_path}"
                    },
                    "position": {
                        "line": line - 1,
                        "character": column - 1
                    },
                    "context": {
                        "includeDeclaration": True
                    }
                }
            }
            
            self._send_message(language, ref_msg)
            response = self._receive_message(language)
            
            references = []
            if response and 'result' in response:
                for ref in response['result']:
                    references.append({
                        'file_path': ref['uri'].replace('file://', ''),
                        'line': ref['range']['start']['line'] + 1,
                        'column': ref['range']['start']['character'] + 1
                    })
            
            return references
            
        except Exception as e:
            logger.error(f"Failed to find references via LSP: {e}")
            return []

    def _send_message(self, language: str, message: dict):
        """Send message to LSP server"""
        if language not in self.processes:
            return
        
        content = json.dumps(message)
        length = len(content)
        header = f"Content-Length: {length}\r\n\r\n"
        
        self.processes[language].stdin.write(header + content)
        self.processes[language].stdin.flush()

    def _receive_message(self, language: str) -> Optional[dict]:
        """Receive message from LSP server"""
        if language not in self.processes:
            return None
        
        # Read content length
        line = self.processes[language].stdout.readline().strip()
        if not line.startswith('Content-Length:'):
            return None
        
        length = int(line.split(':')[1].strip())
        
        # Read empty line
        self.processes[language].stdout.readline()
        
        # Read content
        content = self.processes[language].stdout.read(length)
        return json.loads(content)

    def _detect_language(self, file_path: str) -> Optional[str]:
        """Detect language from file extension"""
        ext_map = {
            '.py': 'python',
            '.js': 'javascript',
            '.ts': 'typescript',
            '.java': 'java',
            '.cpp': 'cpp', '.cc': 'cpp', '.cxx': 'cpp', '.h': 'cpp', '.hpp': 'cpp',
            '.go': 'go',
            '.rs': 'rust'
        }
        
        ext = Path(file_path).suffix.lower()
        return ext_map.get(ext)

    def shutdown(self):
        """Shutdown all LSP servers"""
        for language, process in self.processes.items():
            try:
                # Send shutdown message
                shutdown_msg = {
                    "jsonrpc": "2.0",
                    "id": 999,
                    "method": "shutdown"
                }
                self._send_message(language, shutdown_msg)
                
                # Wait for exit
                process.terminate()
                process.wait(timeout=5)
            except:
                process.kill()
        
        self.processes = {}
        self.servers = {}

# Global LSP client instance
lsp_client = LSPClient()


--- File: C:\Shashank\SaaS\code-rag-debugger\backend\utils\__init__.py ---

from .chunking import chunk_commits, chunk_text
from .file_processing import read_code_files, find_code_files, extract_code_metadata
from .logging import setup_logging, get_logger

__all__ = [
    "chunk_commits",
    "chunk_text",
    "read_code_files",
    "find_code_files",
    "extract_code_metadata",
    "setup_logging",
    "get_logger"
]

